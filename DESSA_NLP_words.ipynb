{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ks-projects-201801.csv', 'ks-projects-201612.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    \n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1533.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000003930</td>\n",
       "      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>2421.00</td>\n",
       "      <td>failed</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2421.00</td>\n",
       "      <td>30000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220.00</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220.00</td>\n",
       "      <td>220.00</td>\n",
       "      <td>45000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283.00</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283.00</td>\n",
       "      <td>1283.00</td>\n",
       "      <td>19500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375.00</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375.00</td>\n",
       "      <td>52375.00</td>\n",
       "      <td>50000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000023410</td>\n",
       "      <td>Support Solar Roasted Coffee &amp; Green Energy!  ...</td>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2014-12-21</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2014-12-01 18:30:44</td>\n",
       "      <td>1205.00</td>\n",
       "      <td>successful</td>\n",
       "      <td>16</td>\n",
       "      <td>US</td>\n",
       "      <td>1205.00</td>\n",
       "      <td>1205.00</td>\n",
       "      <td>1000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000030581</td>\n",
       "      <td>Chaser Strips. Our Strips make Shots their B*tch!</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-03-17</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>2016-02-01 20:05:12</td>\n",
       "      <td>453.00</td>\n",
       "      <td>failed</td>\n",
       "      <td>40</td>\n",
       "      <td>US</td>\n",
       "      <td>453.00</td>\n",
       "      <td>453.00</td>\n",
       "      <td>25000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000034518</td>\n",
       "      <td>SPIN - Premium Retractable In-Ear Headphones w...</td>\n",
       "      <td>Product Design</td>\n",
       "      <td>Design</td>\n",
       "      <td>USD</td>\n",
       "      <td>2014-05-29</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>2014-04-24 18:14:43</td>\n",
       "      <td>8233.00</td>\n",
       "      <td>canceled</td>\n",
       "      <td>58</td>\n",
       "      <td>US</td>\n",
       "      <td>8233.00</td>\n",
       "      <td>8233.00</td>\n",
       "      <td>125000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100004195</td>\n",
       "      <td>STUDIO IN THE SKY - A Documentary Feature Film...</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>2014-07-11 21:55:48</td>\n",
       "      <td>6240.57</td>\n",
       "      <td>canceled</td>\n",
       "      <td>43</td>\n",
       "      <td>US</td>\n",
       "      <td>6240.57</td>\n",
       "      <td>6240.57</td>\n",
       "      <td>65000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID      ...      usd_goal_real\n",
       "0  1000002330      ...            1533.95\n",
       "1  1000003930      ...           30000.00\n",
       "2  1000004038      ...           45000.00\n",
       "3  1000007540      ...            5000.00\n",
       "4  1000011046      ...           19500.00\n",
       "5  1000014025      ...           50000.00\n",
       "6  1000023410      ...            1000.00\n",
       "7  1000030581      ...           25000.00\n",
       "8  1000034518      ...          125000.00\n",
       "9   100004195      ...           65000.00\n",
       "\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/ks-projects-201801.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_data = df.loc[:,'name']\n",
    "names_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def read_questions(row,column_name):\n",
    "    return gensim.utils.simple_preprocess(str(row[column_name]).encode('utf-8'))\n",
    "\n",
    "documents = []\n",
    "un_documents = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['state'] == 'successful':\n",
    "        un_documents.append(read_questions(row,\"name\"))\n",
    "    if row[\"state\"] == 'failed':\n",
    "        documents.append(read_questions(row,\"name\"))\n",
    "\n",
    "print(type(documents))\n",
    "print(type(un_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4038772895153388\n",
      "31000.0\n"
     ]
    }
   ],
   "source": [
    "#simplified_df = df.transform()\n",
    "idx = np.logical_or(df['state']=='successful',df['state']=='failed')\n",
    "docs = df.loc[idx,'name'].values.astype('U')\n",
    "feature_2 = df.loc[idx,'main_category'].values.astype('U')\n",
    "feature_3 = df.loc[idx,'goal'].values\n",
    "\n",
    "targets = df.loc[idx,'state'].values\n",
    "\n",
    "output = np.ones(len(targets),dtype='bool')\n",
    "for i in range(0,len(targets)):\n",
    "    if(targets[i]=='failed'):\n",
    "        output[i] = False\n",
    "\n",
    "# Shows fraction of inputs that have been successful out of the total dataset.\n",
    "print(sum(output)/len(targets))\n",
    "\n",
    "print(feature_3[0]+feature_3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(set(feature_2))\\nfeature_2 = feature_2.reshape(-1,1)\\nfrom sklearn.preprocessing import OneHotEncoder\\nencoder = OneHotEncoder()\\nprint(feature_2.shape)\\nfeature_2 = encoder.fit_transform(feature_2)\\nprint(feature_2.shape)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(set(feature_2))\n",
    "feature_2 = feature_2.reshape(-1,1)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "print(feature_2.shape)\n",
    "feature_2 = encoder.fit_transform(feature_2)\n",
    "print(feature_2.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test = train_test_split(docs, \n",
    "                                        output, feature_2, feature_3,test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60497\n",
      "(66335, 116123)\n"
     ]
    }
   ],
   "source": [
    "# Add the argument 'ngram_range' later to see if the efficiency improves with bigrams - no improvements whatsoever with bigrams in case of simple regressors\n",
    "vect = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "print(vect.vocabulary_['love'])\n",
    "feature_transformer = TfidfTransformer()\n",
    "X_train_tfidf = feature_transformer.fit_transform(X_train_counts)\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = feature_transformer.transform(X_test_counts)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(X_test_tfidf.shape)\\nprint(f2_test.shape)\\nf3_train = f3_train.reshape(-1,1)\\nf3_test = f3_test.reshape(-1,1)\\nprint(f3_test.shape)\\n\\nfrom scipy import sparse\\nX_train_n = sparse.hstack((X_train_tfidf,f2_train,f3_train))\\nX_test_n = sparse.hstack((X_test_tfidf,f2_test,f3_test))\\nprint(X_train_n.shape)\\nprint(X_test_n.shape)\\n#X_train_n = np.concatenate((X_train_tfidf.toarray(),f2_train,f3_train))\\n#X_test_n = np.concatenate((X_test_tfidf.toarray(),f2_test,f3_test))'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional features seemed to decrease the accuracy of prediction\n",
    "'''print(X_test_tfidf.shape)\n",
    "print(f2_test.shape)\n",
    "f3_train = f3_train.reshape(-1,1)\n",
    "f3_test = f3_test.reshape(-1,1)\n",
    "print(f3_test.shape)\n",
    "\n",
    "from scipy import sparse\n",
    "X_train_n = sparse.hstack((X_train_tfidf,f2_train,f3_train))\n",
    "X_test_n = sparse.hstack((X_test_tfidf,f2_test,f3_test))\n",
    "print(X_train_n.shape)\n",
    "print(X_test_n.shape)\n",
    "#X_train_n = np.concatenate((X_train_tfidf.toarray(),f2_train,f3_train))\n",
    "#X_test_n = np.concatenate((X_test_tfidf.toarray(),f2_test,f3_test))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#X_test_n = np.concatenate((X_test_tfidf,f2_test,f3_test))\n",
    "#X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470038441245195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6541946182256727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train_tfidf,y_train)\n",
    "y_pred = lg.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3013997, 3694445)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\n",
    "succ_model = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\n",
    "model.build_vocab(documents)\n",
    "succ_model.build_vocab(un_documents)\n",
    "model.train(sentences=documents, total_examples=len(documents), epochs=model.iter)\n",
    "succ_model.train(sentences=un_documents, total_examples=len(un_documents), epochs=succ_model.iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "count = 0\n",
    "for word in word_vectors.vocab:\n",
    "    if count<10:\n",
    "        print(word)\n",
    "        count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(word_vectors.vocab)\n",
    "#print(model.wv[\"education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apps' 'classroom' 'coding' 'diy' 'electronics' 'gardening' 'hub'\n",
      " 'indoor' 'maker' 'makerspace' 'management' 'network' 'plate' 'platform'\n",
      " 'programming' 'store' 'training' 'upgrade' 'using' 'vertical']\n"
     ]
    }
   ],
   "source": [
    "tmp = model.wv.most_similar(positive=['mobile'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['mobile'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuse' 'addiction' 'dad' 'faith' 'giving' 'having' 'healing' 'hope'\n",
      " 'jesus' 'loss' 'message' 'sad' 'sexual' 'times' 'went']\n"
     ]
    }
   ],
   "source": [
    "tmp = model.wv.most_similar(positive=['love'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['love'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base' 'boardgame' 'combat' 'dungeon' 'miniature' 'printable' 'props'\n",
      " 'retro' 'rpgs' 'systems' 'tabletop' 'tactics' 'terrain' 'tile' 'tiles'\n",
      " 'tokens' 'tower' 'wargaming']\n"
     ]
    }
   ],
   "source": [
    "tmp = model.wv.most_similar(positive=['gaming'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['gaming'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coins' 'cthulhu' 'dungeons' 'fate' 'football' 'forge' 'miniature'\n",
      " 'miniatures' 'module' 'pathfinder' 'rulebook' 'setting' 'sourcebook'\n",
      " 'tokens']\n"
     ]
    }
   ],
   "source": [
    "tmp = model.wv.most_similar(positive=['rpg'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['rpg'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aid' 'ar' 'arcade' 'blocks' 'cardboard' 'cheap' 'coding' 'computer'\n",
      " 'content' 'data' 'developer' 'engine' 'environment' 'freaking' 'hack'\n",
      " 'headset' 'instant' 'programming' 'python' 'windows']\n"
     ]
    }
   ],
   "source": [
    "tmp = model.wv.most_similar(positive=['machine','learning'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['machine','learning'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allegiance' 'comedic' 'crime' 'destroy' 'dystopian' 'erotic'\n",
      " 'fantastical' 'friendship' 'genre' 'identity' 'murder' 'paranormal'\n",
      " 'parody' 'satire' 'serial' 'squad' 'superhero' 'teenage' 'villain' 'ya']\n"
     ]
    }
   ],
   "source": [
    "# Note -> Compatibility is something not found in the dataset names\n",
    "# Dating -> Can occur in two relations: Carbon dating & Dating people\n",
    "tmp = model.wv.most_similar(positive=['dating'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['dating'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absurd' 'bigfoot' 'collide' 'cops' 'destroy' 'emotional' 'facts'\n",
      " 'fantastical' 'friendship' 'genre' 'mormon' 'porn' 'possession' 'quarry'\n",
      " 'satire' 'serial' 'sims' 'sisterhood' 'terrors' 'unconscious']\n"
     ]
    }
   ],
   "source": [
    "# Note -> Compatibility is something not found in the dataset names\n",
    "# Dating -> Can occur in two relations: Carbon dating & Dating people\n",
    "tmp = model.wv.most_similar(positive=['dating','relationships'],topn=20)\n",
    "succ_tmp = succ_model.wv.most_similar(positive=['dating','relationships'],topn=20)\n",
    "\n",
    "unsucc_list = []\n",
    "succ_list = []\n",
    "for i in range(0,len(tmp)):\n",
    "    unsucc_list.append(tmp[i][0])\n",
    "    succ_list.append(succ_tmp[i][0])\n",
    "\n",
    "print(np.setdiff1d(succ_list,unsucc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
