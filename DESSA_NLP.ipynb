{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport gensim\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['ks-projects-201801.csv', 'ks-projects-201612.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ks-projects-201801.csv')\ndf.head(10)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"           ID      ...      usd_goal_real\n0  1000002330      ...            1533.95\n1  1000003930      ...           30000.00\n2  1000004038      ...           45000.00\n3  1000007540      ...            5000.00\n4  1000011046      ...           19500.00\n5  1000014025      ...           50000.00\n6  1000023410      ...            1000.00\n7  1000030581      ...           25000.00\n8  1000034518      ...          125000.00\n9   100004195      ...           65000.00\n\n[10 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>name</th>\n      <th>category</th>\n      <th>main_category</th>\n      <th>currency</th>\n      <th>deadline</th>\n      <th>goal</th>\n      <th>launched</th>\n      <th>pledged</th>\n      <th>state</th>\n      <th>backers</th>\n      <th>country</th>\n      <th>usd pledged</th>\n      <th>usd_pledged_real</th>\n      <th>usd_goal_real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000002330</td>\n      <td>The Songs of Adelaide &amp; Abullah</td>\n      <td>Poetry</td>\n      <td>Publishing</td>\n      <td>GBP</td>\n      <td>2015-10-09</td>\n      <td>1000.0</td>\n      <td>2015-08-11 12:12:28</td>\n      <td>0.00</td>\n      <td>failed</td>\n      <td>0</td>\n      <td>GB</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1533.95</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000003930</td>\n      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n      <td>Narrative Film</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2017-11-01</td>\n      <td>30000.0</td>\n      <td>2017-09-02 04:43:57</td>\n      <td>2421.00</td>\n      <td>failed</td>\n      <td>15</td>\n      <td>US</td>\n      <td>100.00</td>\n      <td>2421.00</td>\n      <td>30000.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000004038</td>\n      <td>Where is Hank?</td>\n      <td>Narrative Film</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2013-02-26</td>\n      <td>45000.0</td>\n      <td>2013-01-12 00:20:50</td>\n      <td>220.00</td>\n      <td>failed</td>\n      <td>3</td>\n      <td>US</td>\n      <td>220.00</td>\n      <td>220.00</td>\n      <td>45000.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000007540</td>\n      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n      <td>Music</td>\n      <td>Music</td>\n      <td>USD</td>\n      <td>2012-04-16</td>\n      <td>5000.0</td>\n      <td>2012-03-17 03:24:11</td>\n      <td>1.00</td>\n      <td>failed</td>\n      <td>1</td>\n      <td>US</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>5000.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000011046</td>\n      <td>Community Film Project: The Art of Neighborhoo...</td>\n      <td>Film &amp; Video</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2015-08-29</td>\n      <td>19500.0</td>\n      <td>2015-07-04 08:35:03</td>\n      <td>1283.00</td>\n      <td>canceled</td>\n      <td>14</td>\n      <td>US</td>\n      <td>1283.00</td>\n      <td>1283.00</td>\n      <td>19500.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1000014025</td>\n      <td>Monarch Espresso Bar</td>\n      <td>Restaurants</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2016-04-01</td>\n      <td>50000.0</td>\n      <td>2016-02-26 13:38:27</td>\n      <td>52375.00</td>\n      <td>successful</td>\n      <td>224</td>\n      <td>US</td>\n      <td>52375.00</td>\n      <td>52375.00</td>\n      <td>50000.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1000023410</td>\n      <td>Support Solar Roasted Coffee &amp; Green Energy!  ...</td>\n      <td>Food</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2014-12-21</td>\n      <td>1000.0</td>\n      <td>2014-12-01 18:30:44</td>\n      <td>1205.00</td>\n      <td>successful</td>\n      <td>16</td>\n      <td>US</td>\n      <td>1205.00</td>\n      <td>1205.00</td>\n      <td>1000.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1000030581</td>\n      <td>Chaser Strips. Our Strips make Shots their B*tch!</td>\n      <td>Drinks</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2016-03-17</td>\n      <td>25000.0</td>\n      <td>2016-02-01 20:05:12</td>\n      <td>453.00</td>\n      <td>failed</td>\n      <td>40</td>\n      <td>US</td>\n      <td>453.00</td>\n      <td>453.00</td>\n      <td>25000.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1000034518</td>\n      <td>SPIN - Premium Retractable In-Ear Headphones w...</td>\n      <td>Product Design</td>\n      <td>Design</td>\n      <td>USD</td>\n      <td>2014-05-29</td>\n      <td>125000.0</td>\n      <td>2014-04-24 18:14:43</td>\n      <td>8233.00</td>\n      <td>canceled</td>\n      <td>58</td>\n      <td>US</td>\n      <td>8233.00</td>\n      <td>8233.00</td>\n      <td>125000.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100004195</td>\n      <td>STUDIO IN THE SKY - A Documentary Feature Film...</td>\n      <td>Documentary</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2014-08-10</td>\n      <td>65000.0</td>\n      <td>2014-07-11 21:55:48</td>\n      <td>6240.57</td>\n      <td>canceled</td>\n      <td>43</td>\n      <td>US</td>\n      <td>6240.57</td>\n      <td>6240.57</td>\n      <td>65000.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_data = df.loc[:,'name']\nnames_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_questions(row,column_name):\n    return gensim.utils.simple_preprocess(str(row[column_name]).encode('utf-8'))\n\ndocuments = []\nun_documents = []\nfor index, row in df.iterrows():\n    if row['state'] == 'successful':\n        un_documents.append(read_questions(row,\"name\"))\n    if row[\"state\"] == 'failed':\n        documents.append(read_questions(row,\"name\"))\n\nprint(type(documents))\nprint(type(un_documents))","execution_count":29,"outputs":[{"output_type":"stream","text":"<class 'list'>\n<class 'list'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#simplified_df = df.transform()\nidx = np.logical_or(df['state']=='successful',df['state']=='failed')\ndocs = df.loc[idx,'name'].values.astype('U')\nfeature_2 = df.loc[idx,'main_category'].values.astype('U')\nfeature_3 = df.loc[idx,'goal'].values\n\ntargets = df.loc[idx,'state'].values\n\noutput = np.ones(len(targets),dtype='bool')\nfor i in range(0,len(targets)):\n    if(targets[i]=='failed'):\n        output[i] = False\n\n# Shows fraction of inputs that have been successful out of the total dataset.\nprint(sum(output)/len(targets))\n\nprint(feature_3[0]+feature_3[1])","execution_count":3,"outputs":[{"output_type":"stream","text":"0.4038772895153388\n31000.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(set(feature_2))\nfeature_2 = feature_2.reshape(-1,1)\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nprint(feature_2.shape)\nfeature_2 = encoder.fit_transform(feature_2)\nprint(feature_2.shape)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test = train_test_split(docs, \n                                        output, feature_2, feature_3,test_size = 0.20, random_state = 12)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the argument 'ngram_range' later to see if the efficiency improves with bigrams - no improvements whatsoever with bigrams in case of simple regressors\nvect = CountVectorizer(min_df=1)\n\nX_train_counts = vect.fit_transform(X_train)\nprint(vect.vocabulary_['love'])\nfeature_transformer = TfidfTransformer()\nX_train_tfidf = feature_transformer.fit_transform(X_train_counts)\nX_test_counts = vect.transform(X_test)\nX_test_tfidf = feature_transformer.transform(X_test_counts)\nprint(X_test_tfidf.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"60497\n(66335, 116123)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional features seemed to not increase the accuracy of prediction by any significant amount\n'''print(X_test_tfidf.shape)\nprint(f2_test.shape)\nf3_train = f3_train.reshape(-1,1)\nf3_test = f3_test.reshape(-1,1)\nprint(f3_test.shape)\n\nfrom scipy import sparse\nX_train_n = sparse.hstack((X_train_tfidf,f2_train,f3_train))\nX_test_n = sparse.hstack((X_test_tfidf,f2_test,f3_test))\nprint(X_train_n.shape)\nprint(X_test_n.shape)\n#X_train_n = np.concatenate((X_train_tfidf.toarray(),f2_train,f3_train))\n#X_test_n = np.concatenate((X_test_tfidf.toarray(),f2_test,f3_test))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#X_test_n = np.concatenate((X_test_tfidf,f2_test,f3_test))\n#X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X_train_tfidf, y_train)\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))","execution_count":30,"outputs":[{"output_type":"stream","text":"0.6470038441245195\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlg = LogisticRegression()\nlg.fit(X_train_tfidf,y_train)\ny_pred = lg.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":15,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"0.6541946182256727\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nlg = LinearSVC(C=0.05)\nlg.fit(X_train_tfidf,y_train)\ny_pred = lg.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":17,"outputs":[{"output_type":"stream","text":"0.6558980930127384\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nbr = Ridge(alpha=2)\nbr.fit(X_train_tfidf,y_train)\ny_pred = br.predict(X_test_tfidf)\nprint(r2_score(y_test,y_pred))","execution_count":14,"outputs":[{"output_type":"stream","text":"0.0876087597782611\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_sample = ['adventure debut project play']\nX_sample = ['instantly']\nX_sample_counts = vect.transform(X_sample)\nX_sample_tfidf = feature_transformer.transform(X_sample_counts)\nprint(X_sample_tfidf.shape)\nprint(br.predict(X_sample_tfidf))","execution_count":28,"outputs":[{"output_type":"stream","text":"(1, 116123)\n[0.50950736]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\nsucc_model = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\nmodel.build_vocab(documents)\nsucc_model.build_vocab(un_documents)\nmodel.train(sentences=documents, total_examples=len(documents), epochs=model.iter)\nsucc_model.train(sentences=un_documents, total_examples=len(un_documents), epochs=succ_model.iter)\n","execution_count":31,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n  import sys\n","name":"stderr"},{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"(3013952, 3694445)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors = model.wv\ncount = 0\nfor word in word_vectors.vocab:\n    if count<10:\n        print(word)\n        count += 1\n    else:\n        break","execution_count":32,"outputs":[{"output_type":"stream","text":"the\nsongs\nof\nadelaide\ngreeting\nfrom\nearth\narts\ncapsule\nfor\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(word_vectors.vocab)\n#print(model.wv[\"education\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['mobile'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['mobile'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":33,"outputs":[{"output_type":"stream","text":"['ar' 'classroom' 'coding' 'diy' 'electronics' 'energy' 'gardening'\n 'hardware' 'hub' 'indoor' 'maker' 'makerspace' 'management' 'network'\n 'plate' 'programming' 'store' 'training' 'upgrade' 'vertical']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['love'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['love'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":34,"outputs":[{"output_type":"stream","text":"['addiction' 'birth' 'equality' 'faith' 'found' 'healing' 'hope' 'illness'\n 'jesus' 'loss' 'muslim' 'replacements' 'sexual' 'spread' 'tell' 'times'\n 'told']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['gaming'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['gaming'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":35,"outputs":[{"output_type":"stream","text":"['builder' 'coins' 'dungeon' 'linux' 'miniature' 'module' 'modules'\n 'printable' 'rpgs' 'strategy' 'tabletop' 'tactics' 'terrain' 'tile'\n 'tiles' 'tokens' 'wargaming']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['machine','learning'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['machine','learning'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":37,"outputs":[{"output_type":"stream","text":"['ai' 'ar' 'arcade' 'blocks' 'cardboard' 'coding' 'computer' 'content'\n 'data' 'developer' 'engine' 'environment' 'programming' 'python' 'racing'\n 'speech' 'tutorials' 'vr' 'windows']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note -> Compatibility is something not found in the dataset names\n# Dating -> Can occur in two relations: Carbon dating & Dating people\ntmp = model.wv.most_similar(positive=['dating'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['dating'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":38,"outputs":[{"output_type":"stream","text":"['allegiance' 'bigfoot' 'crime' 'destroy' 'erotic' 'fantastical' 'furry'\n 'genre' 'guardian' 'humans' 'identity' 'murder' 'otome' 'paranormal'\n 'relationships' 'serial' 'superhero' 'ya' 'yaoi' 'zone']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note -> Compatibility is something not found in the dataset names\n# Dating -> Can occur in two relations: Carbon dating & Dating people\ntmp = model.wv.most_similar(positive=['dating','relationships'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['dating','relationships'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":39,"outputs":[{"output_type":"stream","text":"['alchemist' 'autistic' 'betrayal' 'bigfoot' 'cops' 'corruption' 'destroy'\n 'emotional' 'fantastical' 'friendship' 'furry' 'genre' 'goliath' 'kung'\n 'mormon' 'prevention' 'quarry' 'romcom' 'satire' 'serial']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}