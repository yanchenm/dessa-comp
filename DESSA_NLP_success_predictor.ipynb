{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport gensim\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['ks-projects-201801.csv', 'ks-projects-201612.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ks-projects-201801.csv')\ndf.head(10)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"           ID      ...      usd_goal_real\n0  1000002330      ...            1533.95\n1  1000003930      ...           30000.00\n2  1000004038      ...           45000.00\n3  1000007540      ...            5000.00\n4  1000011046      ...           19500.00\n5  1000014025      ...           50000.00\n6  1000023410      ...            1000.00\n7  1000030581      ...           25000.00\n8  1000034518      ...          125000.00\n9   100004195      ...           65000.00\n\n[10 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>name</th>\n      <th>category</th>\n      <th>main_category</th>\n      <th>currency</th>\n      <th>deadline</th>\n      <th>goal</th>\n      <th>launched</th>\n      <th>pledged</th>\n      <th>state</th>\n      <th>backers</th>\n      <th>country</th>\n      <th>usd pledged</th>\n      <th>usd_pledged_real</th>\n      <th>usd_goal_real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000002330</td>\n      <td>The Songs of Adelaide &amp; Abullah</td>\n      <td>Poetry</td>\n      <td>Publishing</td>\n      <td>GBP</td>\n      <td>2015-10-09</td>\n      <td>1000.0</td>\n      <td>2015-08-11 12:12:28</td>\n      <td>0.00</td>\n      <td>failed</td>\n      <td>0</td>\n      <td>GB</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1533.95</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000003930</td>\n      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n      <td>Narrative Film</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2017-11-01</td>\n      <td>30000.0</td>\n      <td>2017-09-02 04:43:57</td>\n      <td>2421.00</td>\n      <td>failed</td>\n      <td>15</td>\n      <td>US</td>\n      <td>100.00</td>\n      <td>2421.00</td>\n      <td>30000.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000004038</td>\n      <td>Where is Hank?</td>\n      <td>Narrative Film</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2013-02-26</td>\n      <td>45000.0</td>\n      <td>2013-01-12 00:20:50</td>\n      <td>220.00</td>\n      <td>failed</td>\n      <td>3</td>\n      <td>US</td>\n      <td>220.00</td>\n      <td>220.00</td>\n      <td>45000.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000007540</td>\n      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n      <td>Music</td>\n      <td>Music</td>\n      <td>USD</td>\n      <td>2012-04-16</td>\n      <td>5000.0</td>\n      <td>2012-03-17 03:24:11</td>\n      <td>1.00</td>\n      <td>failed</td>\n      <td>1</td>\n      <td>US</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>5000.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000011046</td>\n      <td>Community Film Project: The Art of Neighborhoo...</td>\n      <td>Film &amp; Video</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2015-08-29</td>\n      <td>19500.0</td>\n      <td>2015-07-04 08:35:03</td>\n      <td>1283.00</td>\n      <td>canceled</td>\n      <td>14</td>\n      <td>US</td>\n      <td>1283.00</td>\n      <td>1283.00</td>\n      <td>19500.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1000014025</td>\n      <td>Monarch Espresso Bar</td>\n      <td>Restaurants</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2016-04-01</td>\n      <td>50000.0</td>\n      <td>2016-02-26 13:38:27</td>\n      <td>52375.00</td>\n      <td>successful</td>\n      <td>224</td>\n      <td>US</td>\n      <td>52375.00</td>\n      <td>52375.00</td>\n      <td>50000.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1000023410</td>\n      <td>Support Solar Roasted Coffee &amp; Green Energy!  ...</td>\n      <td>Food</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2014-12-21</td>\n      <td>1000.0</td>\n      <td>2014-12-01 18:30:44</td>\n      <td>1205.00</td>\n      <td>successful</td>\n      <td>16</td>\n      <td>US</td>\n      <td>1205.00</td>\n      <td>1205.00</td>\n      <td>1000.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1000030581</td>\n      <td>Chaser Strips. Our Strips make Shots their B*tch!</td>\n      <td>Drinks</td>\n      <td>Food</td>\n      <td>USD</td>\n      <td>2016-03-17</td>\n      <td>25000.0</td>\n      <td>2016-02-01 20:05:12</td>\n      <td>453.00</td>\n      <td>failed</td>\n      <td>40</td>\n      <td>US</td>\n      <td>453.00</td>\n      <td>453.00</td>\n      <td>25000.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1000034518</td>\n      <td>SPIN - Premium Retractable In-Ear Headphones w...</td>\n      <td>Product Design</td>\n      <td>Design</td>\n      <td>USD</td>\n      <td>2014-05-29</td>\n      <td>125000.0</td>\n      <td>2014-04-24 18:14:43</td>\n      <td>8233.00</td>\n      <td>canceled</td>\n      <td>58</td>\n      <td>US</td>\n      <td>8233.00</td>\n      <td>8233.00</td>\n      <td>125000.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100004195</td>\n      <td>STUDIO IN THE SKY - A Documentary Feature Film...</td>\n      <td>Documentary</td>\n      <td>Film &amp; Video</td>\n      <td>USD</td>\n      <td>2014-08-10</td>\n      <td>65000.0</td>\n      <td>2014-07-11 21:55:48</td>\n      <td>6240.57</td>\n      <td>canceled</td>\n      <td>43</td>\n      <td>US</td>\n      <td>6240.57</td>\n      <td>6240.57</td>\n      <td>65000.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_data = df.loc[:,'name']\nnames_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_questions(row,column_name):\n    return gensim.utils.simple_preprocess(str(row[column_name]).encode('utf-8'))\n\ndocuments = []\nun_documents = []\nfor index, row in df.iterrows():\n    if row['state'] == 'successful':\n        un_documents.append(read_questions(row,\"name\"))\n    if row[\"state\"] == 'failed':\n        documents.append(read_questions(row,\"name\"))\n\nprint(type(documents))\nprint(type(un_documents))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#simplified_df = df.transform()\nidx = np.logical_or(df['state']=='successful',df['state']=='failed')\ndocs = df.loc[idx,'name'].values.astype('U')\nfeature_2 = df.loc[idx,'main_category'].values.astype('U')\nfeature_3 = df.loc[idx,'goal'].values\n\ntargets = df.loc[idx,'state'].values\n\noutput = np.ones(len(targets),dtype='bool')\nfor i in range(0,len(targets)):\n    if(targets[i]=='failed'):\n        output[i] = False\n\n# Shows fraction of inputs that have been successful out of the total dataset.\nprint(sum(output)/len(targets))\n","execution_count":3,"outputs":[{"output_type":"stream","text":"0.4038772895153388\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LSTMs for predicting success \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ntraining_samples = int(0.7*len(docs))\nvalidation_samples = int(0.15*len(docs))\ntest_samples = int(0.15*len(docs))\nmax_words = 100000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(docs)\nsequences = tokenizer.texts_to_sequences(docs)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 50\ndata = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(output)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of labels tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\n\nlabels = labels[indices]\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nk = training_samples + validation_samples\nx_val = data[training_samples: k]\ny_val = labels[training_samples: k]\nl = training_samples + validation_samples + test_samples\nx_test = data[k:l]\ny_test = labels[k:l]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of x_train tensor:', x_train.shape)\nprint('Shape of x_val tensor:', x_val.shape)\nprint('Shape of x_test tensor:', x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, LSTM\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(100, recurrent_dropout = 0.4, dropout = 0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nhistory = model.fit(x_train, y_train,epochs=100,batch_size=128,validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(set(feature_2))\nfeature_2 = feature_2.reshape(-1,1)\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nprint(feature_2.shape)\nfeature_2 = encoder.fit_transform(feature_2)\nprint(feature_2.shape)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test = train_test_split(docs, \n                                        output, feature_2, feature_3,test_size = 0.20, random_state = 12)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the argument 'ngram_range' later to see if the efficiency improves with bigrams - no improvements whatsoever with bigrams in case of simple regressors\nvect = CountVectorizer(min_df=1,ngram_range=(0,4))\n\nX_train_counts = vect.fit_transform(X_train)\nprint(vect.vocabulary_['love'])\nfeature_transformer = TfidfTransformer()\nX_train_tfidf = feature_transformer.fit_transform(X_train_counts)\nX_test_counts = vect.transform(X_test)\nX_test_tfidf = feature_transformer.transform(X_test_counts)\nprint(X_test_tfidf.shape)","execution_count":11,"outputs":[{"output_type":"stream","text":"1082224\n(66335, 2149734)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional features seemed to decrease the accuracy of prediction\n'''print(X_test_tfidf.shape)\nprint(f2_test.shape)\nf3_train = f3_train.reshape(-1,1)\nf3_test = f3_test.reshape(-1,1)\nprint(f3_test.shape)\n\nfrom scipy import sparse\nX_train_n = sparse.hstack((X_train_tfidf,f2_train,f3_train))\nX_test_n = sparse.hstack((X_test_tfidf,f2_test,f3_test))\nprint(X_train_n.shape)\nprint(X_test_n.shape)\n#X_train_n = np.concatenate((X_train_tfidf.toarray(),f2_train,f3_train))\n#X_test_n = np.concatenate((X_test_tfidf.toarray(),f2_test,f3_test))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#X_test_n = np.concatenate((X_test_tfidf,f2_test,f3_test))\n#X_train, X_test, y_train, y_test, f2_train, f2_test, f3_train, f3_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X_train_tfidf, y_train)\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlg = LogisticRegression()\nlg.fit(X_train_tfidf,y_train)\ny_pred = lg.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"0.6605864174266978\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier()\nsgd.fit(X_train_tfidf,y_train)\ny_pred = sgd.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\nsgd = RidgeClassifier()\nsgd.fit(X_train_tfidf,y_train)\ny_pred = sgd.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(500, ),verbose=3)\nmlp.fit(X_train_tfidf,y_train)\ny_pred = mlp.predict(X_test_tfidf)\nprint(accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=40, verbose=5,n_jobs=-1)\nrfc.fit(X_train_tfidf,y_train)\ny_pred = rfc.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\nsucc_model = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\nmodel.build_vocab(documents)\nsucc_model.build_vocab(un_documents)\nmodel.train(sentences=documents, total_examples=len(documents), epochs=model.iter)\nsucc_model.train(sentences=un_documents, total_examples=len(un_documents), epochs=succ_model.iter)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors = model.wv\ncount = 0\nfor word in word_vectors.vocab:\n    if count<10:\n        print(word)\n        count += 1\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(word_vectors.vocab)\n#print(model.wv[\"education\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['mobile'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['mobile'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['love'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['love'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['gaming'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['gaming'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['rpg'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['rpg'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = model.wv.most_similar(positive=['machine','learning'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['machine','learning'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note -> Compatibility is something not found in the dataset names\n# Dating -> Can occur in two relations: Carbon dating & Dating people\ntmp = model.wv.most_similar(positive=['dating'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['dating'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note -> Compatibility is something not found in the dataset names\n# Dating -> Can occur in two relations: Carbon dating & Dating people\ntmp = model.wv.most_similar(positive=['dating','relationships'],topn=20)\nsucc_tmp = succ_model.wv.most_similar(positive=['dating','relationships'],topn=20)\n\nunsucc_list = []\nsucc_list = []\nfor i in range(0,len(tmp)):\n    unsucc_list.append(tmp[i][0])\n    succ_list.append(succ_tmp[i][0])\n\nprint(np.setdiff1d(succ_list,unsucc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}